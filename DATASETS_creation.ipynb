{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11373588,"sourceType":"datasetVersion","datasetId":7120263},{"sourceId":11400831,"sourceType":"datasetVersion","datasetId":7140616},{"sourceId":11488118,"sourceType":"datasetVersion","datasetId":7200854},{"sourceId":11519719,"sourceType":"datasetVersion","datasetId":7224741},{"sourceId":11520195,"sourceType":"datasetVersion","datasetId":7224967}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T17:56:59.437202Z","iopub.execute_input":"2025-04-19T17:56:59.437507Z","iopub.status.idle":"2025-04-19T17:56:59.792587Z","shell.execute_reply.started":"2025-04-19T17:56:59.437481Z","shell.execute_reply":"2025-04-19T17:56:59.791786Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/coral-reef-data/coral_csv/CREMP_Pcover_2023_TaxaGroups.csv\n/kaggle/input/coral-reef-data/coral_csv/CREMP_Stations_2023.csv\n/kaggle/input/coral-reef-data/coral_csv/CREMP_SCOR_Summaries_2023_ConditionCounts.csv\n/kaggle/input/coral-reef-data/coral_csv/CREMP_OCTO_Summaries_2023_MeanHeight.csv\n/kaggle/input/coral-reef-data/coral_csv/CREMP_SCOR_Summaries_2023_LTA.csv\n/kaggle/input/coral-reef-data/coral_csv/CREMP_OCTO_Summaries_2023_Density.csv\n/kaggle/input/coral-reef-data/coral_csv/CREMP_SCOR_Summaries_2023_Density.csv\n/kaggle/input/coral-reef-data/coral_csv/CREMP_SCOR_RawData_2023.csv\n/kaggle/input/coral-reef-data/coral_csv/CREMP_SCOR_Summaries_2023_Counts.csv\n/kaggle/input/coral-reef-data/coral_csv/CREMP_OCTO_RawData_2023.csv\n/kaggle/input/coral-reef-data/coral_csv/CREMP_Temperatures_2023.csv\n/kaggle/input/coral-reef-data/coral_csv/CREMP_Pcover_2023_StonyCoralSpecies.csv\n/kaggle/input/xml-files/xml files/CREMP_SCOR_Summaries_Condition_Counts_Table.xml\n/kaggle/input/xml-files/xml files/CREMP_OCTO_RawData_Table.xml\n/kaggle/input/xml-files/xml files/CREMP_Pcover_Taxa_Groups_Table.xml\n/kaggle/input/xml-files/xml files/CREMP_Pcover_2023_StonyCoralSpecies.csv.xml\n/kaggle/input/xml-files/xml files/CREMP_OCTO_Summaries_Density_Table.xml\n/kaggle/input/xml-files/xml files/CREMP_SCOR_RawData_Table.xml\n/kaggle/input/xml-files/xml files/CREMP_Temperature_Table.xml\n/kaggle/input/xml-files/xml files/CREMP_Stations_2023.csv.xml\n/kaggle/input/xml-files/xml files/CREMP_OCTO_Summaries_MeanHeight_Table.xml\n/kaggle/input/xml-files/xml files/CREMP_SCOR_Summaries_LTA_Table.xml\n/kaggle/input/xml-files/xml files/CREMP_Station_Locations_pnt_Albers.xml\n/kaggle/input/xml-files/xml files/CREMP_SCOR_Summaries_Density_Table.xml\n/kaggle/input/xml-files/xml files/CREMP_SCOR_Summaries_Counts_Table.xml\n/kaggle/input/xml-files/xml files/CREMP_Pcover_Stony_Coral_Species_Table.xml\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# OCTO (Octocorals)\nocto_raw_path = \"/kaggle/input/coral-reef-data/coral_csv/CREMP_OCTO_RawData_2023.csv\"\nocto_density_path = \"/kaggle/input/coral-reef-data/coral_csv/CREMP_OCTO_Summaries_2023_Density.csv\"\nocto_mean_height_path = \"/kaggle/input/coral-reef-data/coral_csv/CREMP_OCTO_Summaries_2023_MeanHeight.csv\"\n\n# SCOR (Stony Corals)\nscor_condition_counts_path = \"/kaggle/input/coral-reef-data/coral_csv/CREMP_SCOR_Summaries_2023_ConditionCounts.csv\"\nscor_counts_path = \"/kaggle/input/coral-reef-data/coral_csv/CREMP_SCOR_Summaries_2023_Counts.csv\"\nscor_lta_path = \"/kaggle/input/coral-reef-data/coral_csv/CREMP_SCOR_Summaries_2023_LTA.csv\"\n\n# Percent Cover\npcover_species_path = \"/kaggle/input/coral-reef-data/coral_csv/CREMP_Pcover_2023_StonyCoralSpecies.csv\"\npcover_taxa_groups_path = \"/kaggle/input/coral-reef-data/coral_csv/CREMP_Pcover_2023_TaxaGroups.csv\"\n\n# Stations & Temperatures\nstations_path = \"/kaggle/input/coral-reef-data/coral_csv/CREMP_Stations_2023.csv\"\ntemperatures_path = \"/kaggle/input/coral-reef-data/coral_csv/CREMP_Temperatures_2023.csv\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T17:56:59.793694Z","iopub.execute_input":"2025-04-19T17:56:59.794120Z","iopub.status.idle":"2025-04-19T17:56:59.800000Z","shell.execute_reply.started":"2025-04-19T17:56:59.794097Z","shell.execute_reply":"2025-04-19T17:56:59.798555Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\n\n# === OCTO (Octocoral) ===\nocto_raw_df = pd.read_csv(\"/kaggle/input/coral-reef-data/coral_csv/CREMP_OCTO_RawData_2023.csv\")\nocto_density_df = pd.read_csv(\"/kaggle/input/coral-reef-data/coral_csv/CREMP_OCTO_Summaries_2023_Density.csv\")\nocto_mean_height_df = pd.read_csv(\"/kaggle/input/coral-reef-data/coral_csv/CREMP_OCTO_Summaries_2023_MeanHeight.csv\")\n\n# === SCOR (Stony Corals) ===\nscor_condition_counts_df = pd.read_csv(\"/kaggle/input/coral-reef-data/coral_csv/CREMP_SCOR_Summaries_2023_ConditionCounts.csv\")\nscor_counts_df = pd.read_csv(\"/kaggle/input/coral-reef-data/coral_csv/CREMP_SCOR_Summaries_2023_Counts.csv\")\nscor_lta_df = pd.read_csv(\"/kaggle/input/coral-reef-data/coral_csv/CREMP_SCOR_Summaries_2023_LTA.csv\")\nscor_density_df=pd.read_csv(\"/kaggle/input/coral-reef-data/coral_csv/CREMP_SCOR_Summaries_2023_Density.csv\")\n\n# === Percent Cover ===\npcover_species_df = pd.read_csv(\"/kaggle/input/coral-reef-data/coral_csv/CREMP_Pcover_2023_StonyCoralSpecies.csv\")\npcover_taxa_groups_df = pd.read_csv(\"/kaggle/input/coral-reef-data/coral_csv/CREMP_Pcover_2023_TaxaGroups.csv\")\n\n# === Station Metadata & Temperatures ===\ndf_stations = pd.read_csv(\"/kaggle/input/coral-reef-data/coral_csv/CREMP_Stations_2023.csv\")\ntemperatures_df = pd.read_csv(\"/kaggle/input/coral-reef-data/coral_csv/CREMP_Temperatures_2023.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:55:28.837355Z","iopub.execute_input":"2025-04-20T18:55:28.837697Z","iopub.status.idle":"2025-04-20T18:55:41.395254Z","shell.execute_reply.started":"2025-04-20T18:55:28.837672Z","shell.execute_reply":"2025-04-20T18:55:41.394246Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"#\ncols = scor_raw_df.columns.tolist()\nidx1 = cols.index(\"Conditions\")\nidx2 = cols.index(\"LTA_cm2\")\ncols[idx1], cols[idx2] = cols[idx2], cols[idx1]\nscor_raw_df = scor_raw_df[cols]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scor_raw_df.to_csv(\"updated_scor_raw_df.csv\", index=False, quoting=1)  # quoting=1 means quote all non-numeric fields\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T14:37:17.861822Z","iopub.execute_input":"2025-04-20T14:37:17.862234Z","iopub.status.idle":"2025-04-20T14:37:18.731345Z","shell.execute_reply.started":"2025-04-20T14:37:17.862209Z","shell.execute_reply":"2025-04-20T14:37:18.730280Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"\n# Sort by StationID and then sciName alphabetically\nsorted_df = scor_raw_df.sort_values(by=[\"StationID\", \"sciName\"]).reset_index(drop=True)\n\n# Save the sorted full DataFrame (optional)\nsorted_df.to_csv(\"scor_raw_df_sorted_by_station_sciName.csv\", index=False)\n\n# View first few rows\nprint(sorted_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:14:25.914119Z","iopub.execute_input":"2025-04-20T11:14:25.914495Z","iopub.status.idle":"2025-04-20T11:14:27.029038Z","shell.execute_reply.started":"2025-04-20T11:14:25.914467Z","shell.execute_reply":"2025-04-20T11:14:27.028064Z"}},"outputs":[{"name":"stdout","text":"   Year       Date Subregion Habitat  SiteID     Site_name  StationID  \\\n0  2011  5/14/2011        LK     BCP      18  Content Keys        181   \n1  2011  5/14/2011        LK     BCP      18  Content Keys        181   \n2  2011  5/14/2011        LK     BCP      18  Content Keys        181   \n3  2011  5/14/2011        LK     BCP      18  Content Keys        181   \n4  2011  5/14/2011        LK     BCP      18  Content Keys        181   \n\n  SPP_CODE              sciName  Diameter_cm  Height_cm  \\\n0     CARB  Cladocora arbuscula            6          3   \n1     CARB  Cladocora arbuscula            6          3   \n2     CARB  Cladocora arbuscula            9          4   \n3     CARB  Cladocora arbuscula            4          3   \n4     CARB  Cladocora arbuscula            4          3   \n\n   Percent_old_mortality  Percent_recent_mortality  Tissue_isolates  \\\n0                    0.0                       0.0              1.0   \n1                    0.0                       0.0              1.0   \n2                    0.0                       0.0              1.0   \n3                    0.0                       0.0              1.0   \n4                    0.0                       0.0              1.0   \n\n  Conditions  LTA_cm2  \n0        NaN    56.54  \n1        NaN    56.54  \n2        NaN   117.92  \n3        NaN    33.83  \n4        NaN    33.83  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Code for  \nimport pandas as pd\nimport re\n\n# Step 1: Load the raw CSV\ndf = pd.read_csv(\"/kaggle/input/coral-reef-data/coral_csv/CREMP_SCOR_RawData_2023.csv\")  # Replace with actual path\n\ncondition_like_cols = [col for col in df.columns if 'Condition' in col or 'Unnamed' in col]\ndef merge_conditions(row):\n    return \";\".join(\n        sorted(set(str(row[col]).strip() for col in condition_like_cols if col in row and pd.notna(row[col])))\n    )\ndf['Cleaned_Conditions'] = df.apply(merge_conditions, axis=1)\n\n# Step 4: Split numbers and valid labels into separate columns\ndef extract_lta_and_conditions(raw):\n    tokens = str(raw).split(';')\n    lta_values = [float(t) for t in tokens if re.match(r'^\\d+(\\.\\d+)?$', t)]\n    condition_labels = [t for t in tokens if t in valid_conditions]\n    return pd.Series({\n        'Condition_List': condition_labels,\n        'LTA_cm2': lta_values[0] if lta_values else None,\n        'Cleaned_Conditions': \";\".join(condition_labels)\n    })\n\nsplit_df = df['Cleaned_Conditions'].apply(extract_lta_and_conditions)\n\n# Step 5: Update DataFrame\ndf[['Condition_List', 'LTA_cm2', 'Cleaned_Conditions']] = split_df\n\n# Optional: Save the result\ndf.to_csv(\"cleaned_conditions_with_lta.csv\", index=False)\nprint(\"✅ Cleaned and saved to 'cleaned_conditions_with_lta.csv'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:45:27.526345Z","iopub.execute_input":"2025-04-20T11:45:27.526982Z","iopub.status.idle":"2025-04-20T11:45:53.561482Z","shell.execute_reply.started":"2025-04-20T11:45:27.526956Z","shell.execute_reply":"2025-04-20T11:45:53.560640Z"}},"outputs":[{"name":"stdout","text":"✅ Cleaned and saved to 'cleaned_conditions_with_lta.csv'\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# code to combine scor_rw df scor_lta_df and scor_density df into one df for easy analysis\n# Reinitialize and apply corrected full logic to ensure scor_lta aligns with correct station\n\n# Reset scor_raw_df\nscor_raw_df = scor_raw_df.reset_index(drop=True)\nif 'scor_lta' not in scor_raw_df.columns:\n    scor_raw_df['scor_lta'] = None\n\n# Initialize indices\nj = 0\ni = 0\nk = 0\n\n# Apply corrected logic with StationID comparison before incrementing j\nwhile i < len(scor_lta_df) and j < len(scor_condition_counts_df):\n    # Get species name in correct format\n    species = scor_condition_counts_df.iloc[j]['sciName'].replace(\" \", \"_\")\n    count = int(scor_condition_counts_df.iloc[j]['Count']) if pd.notna(scor_condition_counts_df.iloc[j]['Count']) else 0\n\n    # Assign LTA values\n    for _ in range(count):\n        if k < len(scor_raw_df):\n            if species in scor_lta_df.columns:\n                scor_raw_df.at[k, 'scor_lta'] = scor_lta_df.at[i, species]\n                scor_raw_df.at[k, 'density'] = scor_density_df.at[i, species]\n\n        k += 1\n\n    # Check station ID difference BEFORE incrementing j\n    if j + 1 < len(scor_condition_counts_df):\n        current_station = scor_condition_counts_df.at[j, 'StationID']\n        next_station = scor_condition_counts_df.at[j + 1, 'StationID']\n        if current_station != next_station:\n            i += 1\n    else:\n        i += 1  # end of file, increment i\n\n    j += 1  # move to next row in condition counts\n\n# Check output\nnon_null_count = scor_raw_df['scor_lta'].notna().sum()\nsample_output = scor_raw_df[['sciName', 'scor_lta']].tail(20)\n\nnon_null_count, sample_output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:55:57.199442Z","iopub.execute_input":"2025-04-20T18:55:57.200062Z","iopub.status.idle":"2025-04-20T18:56:05.710800Z","shell.execute_reply.started":"2025-04-20T18:55:57.200035Z","shell.execute_reply":"2025-04-20T18:56:05.709916Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(136061,\n                            sciName scor_lta\n 136041         Siderastrea siderea   2707.1\n 136042  Undaria agaricites complex   2707.1\n 136043         Siderastrea siderea   2707.1\n 136044         Siderastrea siderea   2707.1\n 136045          Porites astreoides   2707.1\n 136046  Undaria agaricites complex   2707.1\n 136047   Stephanocoenia intersepta   2707.1\n 136048         Siderastrea siderea   2707.1\n 136049         Siderastrea siderea   2707.1\n 136050         Siderastrea siderea   2707.1\n 136051         Orbicella faveolata   2707.1\n 136052          Porites astreoides   2707.1\n 136053          Porites astreoides   2707.1\n 136054           Manicina areolata   2707.1\n 136055         Siderastrea siderea   2707.1\n 136056         Siderastrea siderea    459.4\n 136057         Siderastrea siderea    459.4\n 136058         Siderastrea siderea    459.4\n 136059         Siderastrea siderea     70.1\n 136060         Siderastrea siderea     70.1)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"#applying internal sorting in scor_raw df to match expectation of scor_lta and scor_density_df\nsorted_chunks = []\nstart_idx = 0\nwhile start_idx < len(df):\n    current_station = scor_raw_df.loc[start_idx, 'StationID']\n    end_idx = start_idx\n    while end_idx + 1 < len(scor_raw_df) and scor_raw_df.loc[end_idx + 1, 'StationID'] == current_station:\n        end_idx += 1\n    block = scor_raw_df.iloc[start_idx:end_idx+1].sort_values(by=\"sciName\")\n    sorted_chunks.append(block)\n    start_idx = end_idx + 1\nscor_raw_df = pd.concat(sorted_chunks).reset_index(drop=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:31:19.573041Z","iopub.execute_input":"2025-04-20T11:31:19.573782Z","iopub.status.idle":"2025-04-20T11:31:21.821985Z","shell.execute_reply.started":"2025-04-20T11:31:19.573723Z","shell.execute_reply":"2025-04-20T11:31:21.821135Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Save the updated DataFrame to a CSV file\nscor_raw_df.to_csv(\"updated_scor_raw_df.csv\", index=False,quoting=1)\ntemperatures_df.to_csv(\"updated_temperatures_df.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:56:08.791038Z","iopub.execute_input":"2025-04-20T18:56:08.791351Z","iopub.status.idle":"2025-04-20T18:56:11.576778Z","shell.execute_reply.started":"2025-04-20T18:56:08.791326Z","shell.execute_reply":"2025-04-20T18:56:11.575705Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:56:05.712426Z","iopub.execute_input":"2025-04-20T18:56:05.712839Z","iopub.status.idle":"2025-04-20T18:56:06.695849Z","shell.execute_reply.started":"2025-04-20T18:56:05.712811Z","shell.execute_reply":"2025-04-20T18:56:06.695050Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#adding temp in scor_raw_df by making correlations map form tempratures_df and adding to scor_raw df\n# Step 1: Parse the Date column in ISO format (yyyy-mm-dd)\nscor_raw_df['Date'] = pd.to_datetime(scor_raw_df['Date'])\n\n# Step 2: Extract Year, Month, Day\nscor_raw_df['Year'] = scor_raw_df['Date'].dt.year\nscor_raw_df['Month'] = scor_raw_df['Date'].dt.month\nscor_raw_df['Day'] = scor_raw_df['Date'].dt.day\n\n# Step 3: Create the key for matching in both dataframes\nscor_raw_df['key'] = list(zip(scor_raw_df['SiteID'], scor_raw_df['Year'], scor_raw_df['Month'], scor_raw_df['Day']))\ntemperatures_df['key'] = list(zip(temperatures_df['SiteID'], temperatures_df['Year'], temperatures_df['Month'], temperatures_df['Day']))\n\n# Step 4: Build a mapping dictionary and apply it\ntemp_map = dict(zip(temperatures_df['key'], temperatures_df['TempC']))\nscor_raw_df['TempC'] = scor_raw_df['key'].map(temp_map)\n\n# Step 5: Drop helper column\nscor_raw_df.drop(columns=['key'], inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:56:06.696778Z","iopub.execute_input":"2025-04-20T18:56:06.697091Z","iopub.status.idle":"2025-04-20T18:56:07.712271Z","shell.execute_reply.started":"2025-04-20T18:56:06.697065Z","shell.execute_reply":"2025-04-20T18:56:07.711017Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"for i in scor_raw_df.length\n    j=assign column number of conditions column\n    k=assign column number of conditions column\n    while(scor_raw_df[i].iloc[j]==string)\n        #make a list of conditions in conditions column e\n    scor_raw_df[i].iloc[k+1]=scor_raw_df[i].iloc[j]\n         ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Detect all columns that might contain part of a condition\ncondition_like_cols = [col for col in scor_raw_df.columns if 'Condition' in col or 'Unnamed' in col]\n\n# Step 2: For each row, combine all non-null string values from these columns into a single list of conditions\ndef collect_conditions(row):\n    condition_list = []\n    for col in condition_like_cols:\n        val = str(row[col]).strip()\n        if val and val.lower() != 'nan':\n            condition_list.append(val)\n    return list(sorted(set(condition_list)))  # deduplicated\n\n# Apply to all rows\nscor_raw_df['Condition_List'] = scor_raw_df.apply(collect_conditions, axis=1)\nscor_raw_df['Num_Conditions'] = scor_raw_df['Condition_List'].apply(len)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T18:23:20.909315Z","iopub.execute_input":"2025-04-19T18:23:20.909556Z","iopub.status.idle":"2025-04-19T18:23:21.936685Z","shell.execute_reply.started":"2025-04-19T18:23:20.909537Z","shell.execute_reply":"2025-04-19T18:23:21.935947Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"scor_raw_df['Condition_List'] = scor_raw_df['Conditions'].astype(str).apply(\n    lambda x: [s.strip() for s in x.split(';') if s.strip() and s.lower() != 'nan']\n)\n\n# Update the number of conditions\nscor_raw_df['Num_Conditions'] = scor_raw_df['Condition_List'].apply(len)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T18:26:29.439718Z","iopub.execute_input":"2025-04-19T18:26:29.440104Z","iopub.status.idle":"2025-04-19T18:26:29.677574Z","shell.execute_reply.started":"2025-04-19T18:26:29.440080Z","shell.execute_reply":"2025-04-19T18:26:29.676898Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"#applying sorting by station names in octo_raw df(misnamed as scor here as i had to implement the logic from above)\nimport pandas as pd\n\n# Load the CSV file\nscor_raw_df = pd.read_csv(\"/kaggle/input/coral-reef-data/coral_csv/CREMP_OCTO_RawData_2023.csv\")\n\n# Initialize list to store sorted chunks\nsorted_chunks = []\nstart_idx = 0\n\n# Iterate through the DataFrame to sort chunks with the same StationID\nwhile start_idx < len(scor_raw_df):\n    current_station = scor_raw_df.loc[start_idx, 'StationID']\n    end_idx = start_idx\n    while end_idx + 1 < len(scor_raw_df) and scor_raw_df.loc[end_idx + 1, 'StationID'] == current_station:\n        end_idx += 1\n    # Sort the block by 'sciName'\n    block = scor_raw_df.iloc[start_idx:end_idx + 1].sort_values(by=\"sciName\")\n    sorted_chunks.append(block)\n    start_idx = end_idx + 1\n\n# Concatenate all the sorted blocks and reset the index\nscor_raw_df_sorted = pd.concat(sorted_chunks).reset_index(drop=True)\n\n# Optional: Save the sorted DataFrame to a new CSV file\nscor_raw_df_sorted.to_csv(\"CREMP_OCTO_SortedBy_Station_sciName.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:23:06.376767Z","iopub.execute_input":"2025-04-22T14:23:06.377081Z","iopub.status.idle":"2025-04-22T14:23:08.486884Z","shell.execute_reply.started":"2025-04-22T14:23:06.377057Z","shell.execute_reply":"2025-04-22T14:23:08.486212Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#sorting columns in octo height df by alphabetic order\nimport pandas as pd\n\n# Load the CSV files\nocto_height_df = pd.read_csv(\"/kaggle/input/coral-reef-data/coral_csv/CREMP_OCTO_Summaries_2023_MeanHeight.csv\")\n# Define the species columns to sort\nspecies_cols_to_sort = [\n    'Eunicea_calyculata',\n    'Gorgonia_ventalina',\n    'Pseudopterogorgia_americana',\n    'Pseudopterogorgia_bipinnata',\n    'Eunicea_flexuosa',\n    'Pseudoplexaura_porosa'\n]\n\n# Function to reorder DataFrame columns with species sorted alphabetically\ndef sort_species_columns(df, species_cols):\n    fixed_cols = [col for col in df.columns if col not in species_cols]\n    sorted_species_cols = sorted(species_cols)\n    return df[fixed_cols + sorted_species_cols]\n\n# Apply sorting to both DataFrames\nocto_height_df = sort_species_columns(octo_height_df, species_cols_to_sort)\n\n# Save sorted DataFrames to CSV files\nocto_height_df.to_csv(\"Sorted_OCTO_Height_2023.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:30:22.568795Z","iopub.execute_input":"2025-04-22T14:30:22.569099Z","iopub.status.idle":"2025-04-22T14:30:22.631621Z","shell.execute_reply.started":"2025-04-22T14:30:22.569077Z","shell.execute_reply":"2025-04-22T14:30:22.630882Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#sorting columns in octo density df by alphabetic order\nimport pandas as pd\n\n# Step 1: Load the octocoral density data\nocto_density_df = pd.read_csv(\"/kaggle/input/coral-reef-data/coral_csv/CREMP_OCTO_Summaries_2023_Density.csv\")\n\n# Step 2: Define species columns to sort alphabetically\nspecies_cols_to_sort = [\n    'Eunicea_calyculata',\n    'Gorgonia_ventalina',\n    'Pseudopterogorgia_americana',\n    'Pseudopterogorgia_bipinnata',\n    'Eunicea_flexuosa',\n    'Pseudoplexaura_porosa'\n]\n\n# Step 3: Identify non-species columns (excluding 'Total_Octocorals')\nnon_species_cols = [col for col in octo_density_df.columns if col not in species_cols_to_sort + ['Total_Octocorals']]\nsorted_species_cols = sorted(species_cols_to_sort)\n\n# Step 4: Reconstruct the DataFrame with species columns sorted and Total_Octocorals moved to end\nfinal_columns = non_species_cols + sorted_species_cols + ['Total_Octocorals']\nocto_density_df = octo_density_df[final_columns]\n\n# Step 5: Save the cleaned DataFrame to a CSV file\nocto_density_df.to_csv(\"Sorted_OCTO_Density_With_Total_End.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:40:01.969720Z","iopub.execute_input":"2025-04-22T14:40:01.970023Z","iopub.status.idle":"2025-04-22T14:40:01.989943Z","shell.execute_reply.started":"2025-04-22T14:40:01.970000Z","shell.execute_reply":"2025-04-22T14:40:01.989102Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#adding density and height in octo dataset\nimport pandas as pd\n\n# Load data\nocto_sorted_df = pd.read_csv(\"/kaggle/working/CREMP_OCTO_Fixed_Sorted.csv\")\nocto_density_df = pd.read_csv(\"/kaggle/input/octo-complete-new/Sorted_OCTO_Density_With_Total_End.csv\")\nocto_height_df = pd.read_csv(\"/kaggle/input/octo-complete-new/Sorted_OCTO_Height_2023.csv\")\n\n# Step 1: Normalize sciName\nocto_sorted_df['sciName'] = octo_sorted_df['sciName'].str.strip().str.replace(' ', '_')\n\n# Step 2: Replace known abbreviations\nname_map = {\n    'P._bipinnata/P._kallos': 'Pseudopterogorgia_bipinnata',\n    'P._americana': 'Pseudopterogorgia_americana',\n    'E._flexuosa': 'Eunicea_flexuosa',\n    'E._calyculata': 'Eunicea_calyculata',\n    'G._ventalina': 'Gorgonia_ventalina',\n    'P._porosa': 'Pseudoplexaura_porosa'\n}\nocto_sorted_df['sciName'] = octo_sorted_df['sciName'].replace(name_map)\n\n# Step 3: Rename height columns to match\nocto_height_df = octo_height_df.rename(columns={\n    'stationid': 'StationID',\n    'siteid': 'SiteID',\n    'FirstOfDate': 'Date',\n    'subRegionID': 'Subregion'\n})\n\n# Step 4: Ensure types are consistent\nocto_sorted_df['StationID'] = octo_sorted_df['StationID'].astype(int)\nocto_sorted_df['Year'] = octo_sorted_df['Year'].astype(int)\nocto_density_df['StationID'] = octo_density_df['StationID'].astype(int)\nocto_density_df['Year'] = octo_density_df['Year'].astype(int)\nocto_height_df['StationID'] = octo_height_df['StationID'].astype(int)\nocto_height_df['Year'] = octo_height_df['Year'].astype(int)\n\n# Step 5: Melt density and height DataFrames\nmelted_density_df = octo_density_df.melt(\n    id_vars=['Year', 'StationID'],\n    var_name='sciName',\n    value_name='Density'\n)\n\nmelted_height_df = octo_height_df.melt(\n    id_vars=['Year', 'StationID'],\n    var_name='sciName',\n    value_name='Height'\n)\n\n# Step 6: Merge both into the sorted dataframe\nfinal_df = pd.merge(\n    octo_sorted_df,\n    melted_density_df,\n    on=['Year', 'StationID', 'sciName'],\n    how='left'\n)\n\nfinal_df = pd.merge(\n    final_df,\n    melted_height_df,\n    on=['Year', 'StationID', 'sciName'],\n    how='left'\n)\n\n# Step 7: Save final output with both Density and Height\nfinal_df.to_csv(\"/kaggle/working/Octo_Sorted_With_Corrected_Density_And_Height.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T21:49:22.320720Z","iopub.execute_input":"2025-04-22T21:49:22.321071Z","iopub.status.idle":"2025-04-22T21:49:23.278128Z","shell.execute_reply.started":"2025-04-22T21:49:22.321046Z","shell.execute_reply":"2025-04-22T21:49:23.277278Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\n\n# Load data\ndf = pd.read_csv(\"/kaggle/input/octo-complete-new/CREMP_OCTO_SortedBy_Station_sciName.csv\")\n\n# Convert StationID to numeric\ndf['StationID'] = pd.to_numeric(df['StationID'], errors='coerce')\n\n# Sort by Year, SiteID, then StationID and sciName\ndf_sorted = df.sort_values(by=['Year', 'SiteID', 'StationID', 'sciName']).reset_index(drop=True)\n\n# Save the fixed version\ndf_sorted.to_csv(\"CREMP_OCTO_Fixed_Sorted.csv\", index=False)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T21:39:03.504145Z","iopub.execute_input":"2025-04-22T21:39:03.504801Z","iopub.status.idle":"2025-04-22T21:39:04.197554Z","shell.execute_reply.started":"2025-04-22T21:39:03.504774Z","shell.execute_reply":"2025-04-22T21:39:04.196686Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"adding temp in scor_raw dats et\nimport pandas as pd\n\n# Load datasets\nscor_raw_df = pd.read_csv(\"/kaggle/working/Octo_Sorted_With_Corrected_Density_And_Height.csv\")\ntemperatures_df = pd.read_csv(\"/kaggle/input/temprature-df/updated_temperatures_df.csv\")\n\n# Step 1: Parse date column in scor_raw_df\nscor_raw_df['Date'] = pd.to_datetime(scor_raw_df['Date'], errors='coerce')\n\n# Step 2: Extract Year, Month, Day\nscor_raw_df['Year'] = scor_raw_df['Date'].dt.year\nscor_raw_df['Month'] = scor_raw_df['Date'].dt.month\nscor_raw_df['Day'] = scor_raw_df['Date'].dt.day\n\n# Step 3: Create keys for matching\nscor_raw_df['key'] = list(zip(scor_raw_df['SiteID'], scor_raw_df['Year'], scor_raw_df['Month'], scor_raw_df['Day']))\ntemperatures_df['key'] = list(zip(temperatures_df['SiteID'], temperatures_df['Year'], temperatures_df['Month'], temperatures_df['Day']))\n\n# Step 4: Map TempC using key\ntemp_map = dict(zip(temperatures_df['key'], temperatures_df['TempC']))\nscor_raw_df['TempC'] = scor_raw_df['key'].map(temp_map)\n\n# Step 5: Drop helper column\nscor_raw_df.drop(columns=['key'], inplace=True)\n\n# Step 6: Save the final result\nscor_raw_df.to_csv(\"/kaggle/working/Octo_Data_With_TempC.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T21:55:33.906649Z","iopub.execute_input":"2025-04-22T21:55:33.907185Z","iopub.status.idle":"2025-04-22T21:55:36.116194Z","shell.execute_reply.started":"2025-04-22T21:55:33.907159Z","shell.execute_reply":"2025-04-22T21:55:36.115337Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"for run in $(cat accession_list.txt); do\n    fasterq-dump $run\ndone\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}