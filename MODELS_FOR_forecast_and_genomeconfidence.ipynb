{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11373588,"sourceType":"datasetVersion","datasetId":7120263},{"sourceId":11501912,"sourceType":"datasetVersion","datasetId":7211027},{"sourceId":11505168,"sourceType":"datasetVersion","datasetId":7213573},{"sourceId":11525915,"sourceType":"datasetVersion","datasetId":7228647}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom statsmodels.tsa.arima.model import ARIMA\nimport pandas as pd\ndf=pd.read_csv('/kaggle/input/scor-dataset/updated_scor_raw_df (22).csv')\n# Step 1: Select one SiteID with sufficient data\nsite_data = df[df['SiteID'] == df['SiteID'].value_counts().idxmax()]\n\n# Step 2: Aggregate scor_lta by year (average per year per site)\nts = site_data.groupby('Year')['scor_lta'].mean().sort_index()\n\n# Step 3: Fit ARIMA model (order to be selected based on simplicity for now)\nmodel = ARIMA(ts, order=(1,1,1))\nfitted_model = model.fit()\n\n# Step 4: Forecast next 5 years\nforecast = fitted_model.forecast(steps=5)\n\n# Plotting\nplt.figure(figsize=(10,5))\nplt.plot(ts, label='Observed')\nplt.plot(range(ts.index.max()+1, ts.index.max()+6), forecast, label='Forecast', linestyle='--')\nplt.xlabel(\"Year\")\nplt.ylabel(\"Average scor_lta\")\nplt.title(\"ARIMA Forecast for Live Tissue Area (scor_lta)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom prophet import Prophet\nimport matplotlib.pyplot as plt\n\n# Load your dataset\n\n# Filter for a single SiteID\nsite_df = df[df[\"SiteID\"] == 18]\n\n# Group by year and take mean of scor_lta\nprophet_df = site_df.groupby(\"Year\")[\"scor_lta\"].mean().reset_index()\nprophet_df = prophet_df.rename(columns={\"Year\": \"ds\", \"scor_lta\": \"y\"})\nprophet_df[\"ds\"] = pd.to_datetime(prophet_df[\"ds\"], format=\"%Y\")\n\n# Initialize Prophet model\nmodel = Prophet(yearly_seasonality=False)\nmodel.fit(prophet_df)\n\n# Forecast next 5 years\nfuture = model.make_future_dataframe(periods=5, freq='Y')\nforecast = model.predict(future)\n\n# Plot forecast\nfig1 = model.plot(forecast)\nfig1.suptitle(\"Prophet Forecast for scor_lta (SiteID 18)\", fontsize=14)\n\n# Plot trend and components\nfig2 = model.plot_components(forecast)\nfig2.suptitle(\"Prophet Components (Trend, Seasonality)\", fontsize=14)\n\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from statsmodels.tsa.arima.model import ARIMA\nimport matplotlib.pyplot as plt\n\n# Use the overall trend data again\nts_overall = df.groupby(\"Year\")[\"scor_lta\"].mean().sort_index()\nts_overall.index = pd.to_datetime(ts_overall.index, format=\"%Y\")\n\n# Fit ARIMA model (basic example with order=(1,1,1))\nmodel = ARIMA(ts_overall, order=(1, 1, 1))\nfitted_model = model.fit()\n\n# Forecast next 5 years\nforecast_arima = fitted_model.forecast(steps=5)\n\n# Prepare future years for x-axis\nfuture_years = pd.date_range(start=ts_overall.index[-1] + pd.DateOffset(years=1), periods=5, freq='Y')\n\n# Plot the results\nplt.figure(figsize=(10, 5))\nplt.plot(ts_overall, label='Observed')\nplt.plot(future_years, forecast_arima, label='Forecast (ARIMA)', linestyle='--')\nplt.title(\"ARIMA Forecast: General Trend of Coral scor_lta Across All Sites\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Average scor_lta\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom prophet import Prophet\nimport matplotlib.pyplot as plt\n\n# Load your data\ndf = pd.read_csv(\"/kaggle/input/scor-dataset/updated_scor_raw_df (22).csv\")\n\n# Compute average scor_lta per year across all sites\noverall_trend_df = df.groupby(\"Year\")[\"scor_lta\"].mean().reset_index()\noverall_trend_df = overall_trend_df.rename(columns={\"Year\": \"ds\", \"scor_lta\": \"y\"})\noverall_trend_df[\"ds\"] = pd.to_datetime(overall_trend_df[\"ds\"], format=\"%Y\")\n\n# Initialize Prophet\nmodel = Prophet(yearly_seasonality=False)\nmodel.fit(overall_trend_df)\n\n# Forecast next 5 years\nfuture = model.make_future_dataframe(periods=5, freq='Y')\nforecast = model.predict(future)\n\n# Plot forecast\nfig1 = model.plot(forecast)\nfig1.suptitle(\"Prophet Forecast: General Trend of Coral scor_lta Across All Sites\", fontsize=14)\n\n# Plot trend components\nfig2 = model.plot_components(forecast)\nfig2.suptitle(\"Prophet Components (General Trend)\", fontsize=14)\n\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, GRU, Dense, Masking\n\n# Load data\ndf = pd.read_csv(\"/kaggle/input/scor-dataset/updated_scor_raw_df (22).csv\")\n\n# Aggregate by SiteID and Year\nagg_df = df.groupby(['SiteID', 'Year']).agg({\n    'scor_lta': 'mean',\n    'TempC': 'mean'\n}).reset_index()\n\n# Handle missing values\nagg_df = agg_df.dropna(subset=['scor_lta'])\nagg_df['TempC'] = agg_df.groupby('SiteID')['TempC'].transform(lambda x: x.fillna(x.mean()))\nagg_df['TempC'] = agg_df['TempC'].fillna(agg_df['TempC'].mean())\n\n# Convert to sequences per site\nsite_sequences = []\nfor _, group in agg_df.groupby('SiteID'):\n    group = group.sort_values('Year')\n    site_sequences.append(group[['scor_lta', 'TempC']].values)\n\n# Prepare padded input/output sequences\nX, y = [], []\nfor seq in site_sequences:\n    if len(seq) < 3:\n        continue\n    X.append(seq[:-1])\n    y.append(seq[-1][0])  # predict scor_lta for last year\n\nmax_seq_len = max(len(s) for s in X)\nX_padded = pad_sequences(X, maxlen=max_seq_len, padding='pre', dtype='float32')\n\n# Normalize\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_padded.reshape(-1, 2)).reshape(X_padded.shape)\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, np.array(y), test_size=0.2, random_state=42)\n\n# Define LSTM model\ndef build_lstm():\n    model = Sequential([\n        Masking(mask_value=0.0, input_shape=(X_train.shape[1], 2)),\n        LSTM(64),\n        Dense(1)\n    ])\n    model.compile(optimizer='adam', loss='mse')\n    return model\n\n# Define GRU model\ndef build_gru():\n    model = Sequential([\n        Masking(mask_value=0.0, input_shape=(X_train.shape[1], 2)),\n        GRU(64),\n        Dense(1)\n    ])\n    model.compile(optimizer='adam', loss='mse')\n    return model\n\n# Train LSTM\nlstm_model = build_lstm()\nlstm_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=2)\n\n# Train GRU\ngru_model = build_gru()\ngru_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Masking\nfrom tqdm.keras import TqdmCallback\nimport math\n\n# Load data\ndf = pd.read_csv(\"/kaggle/input/scor-dataset/updated_scor_raw_df (22).csv\")\n\n# Aggregate and select features\nagg_df = df.groupby(['SiteID', 'Year']).agg({\n    'scor_lta': 'mean',\n    'TempC': 'mean',\n    'density': 'mean',\n    'LTA_cm2': 'mean'\n}).reset_index()\n\n# Handle missing\nagg_df = agg_df.dropna(subset=['scor_lta'])\nfor col in ['TempC', 'density', 'LTA_cm2']:\n    agg_df[col] = agg_df.groupby('SiteID')[col].transform(lambda x: x.fillna(x.mean()))\n    agg_df[col] = agg_df[col].fillna(agg_df[col].mean())\n\n# Prepare rolling window sequences\nX, y = [], []\nwindow = 5  # sequence length\n\nfor _, group in agg_df.groupby(\"SiteID\"):\n    group = group.sort_values(\"Year\")\n    data = group[['scor_lta', 'TempC', 'density', 'LTA_cm2']].values\n\n    for i in range(len(data) - window):\n        X.append(data[i:i+window])\n        y.append(np.log1p(data[i+window][0]))  # log-transform scor_lta target\n\n# Pad and normalize\nX = np.array(X)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X.reshape(-1, 4)).reshape(X.shape)\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, np.array(y), test_size=0.2, random_state=42)\n\n# Define LSTM model\ndef build_lstm():\n    model = Sequential([\n        Masking(mask_value=0.0, input_shape=(X_train.shape[1], X_train.shape[2])),\n        LSTM(64, return_sequences=True),\n        Dropout(0.3),\n        LSTM(64),\n        Dense(1)\n    ])\n    model.compile(optimizer='adam', loss='mse')\n    return model\n\n# Define GRU model\ndef build_gru():\n    model = Sequential([\n        Masking(mask_value=0.0, input_shape=(X_train.shape[1], X_train.shape[2])),\n        GRU(64, return_sequences=True),\n        Dropout(0.3),\n        GRU(64),\n        Dense(1)\n    ])\n    model.compile(optimizer='adam', loss='mse')\n    return model\n\n# Train LSTM\nprint(\"Training LSTM model...\")\nlstm_model = build_lstm()\nlstm_model.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    epochs=50,\n    verbose=0,\n    callbacks=[TqdmCallback(verbose=1)]\n)\n\n# Evaluate LSTM\nlstm_preds = lstm_model.predict(X_test)\nlstm_preds = np.expm1(lstm_preds).flatten()\ny_true = np.expm1(y_test)\n\nlstm_mae = mean_absolute_error(y_true, lstm_preds)\nlstm_rmse = math.sqrt(mean_squared_error(y_true, lstm_preds))\nprint(f\"\\n📈 LSTM MAE: {lstm_mae:.2f}, RMSE: {lstm_rmse:.2f}\")\n\n# Train GRU\nprint(\"\\nTraining GRU model...\")\ngru_model = build_gru()\ngru_model.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    epochs=50,\n    verbose=0,\n    callbacks=[TqdmCallback(verbose=1)]\n)\n\n# Evaluate GRU\ngru_preds = gru_model.predict(X_test)\ngru_preds = np.expm1(gru_preds).flatten()\n\ngru_mae = mean_absolute_error(y_true, gru_preds)\ngru_rmse = math.sqrt(mean_squared_error(y_true, gru_preds))\nprint(f\"\\n📈 GRU MAE: {gru_mae:.2f}, RMSE: {gru_rmse:.2f}\")\nfrom sklearn.metrics import r2_score\n\ndef smape(y_true, y_pred):\n    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n\ndef mape(y_true, y_pred):\n    return 100 * np.mean(np.abs((y_true - y_pred) / y_true))\n\n# Already computed y_true, lstm_preds, gru_preds\n\nprint(\"\\n🔍 LSTM Metrics:\")\nprint(f\"MAE  : {mean_absolute_error(y_true, lstm_preds):.2f}\")\nprint(f\"RMSE : {math.sqrt(mean_squared_error(y_true, lstm_preds)):.2f}\")\nprint(f\"MAPE : {mape(y_true, lstm_preds):.2f}%\")\nprint(f\"SMAPE: {smape(y_true, lstm_preds):.2f}%\")\nprint(f\"R²   : {r2_score(y_true, lstm_preds):.4f}\")\n\nprint(\"\\n🔍 GRU Metrics:\")\nprint(f\"MAE  : {mean_absolute_error(y_true, gru_preds):.2f}\")\nprint(f\"RMSE : {math.sqrt(mean_squared_error(y_true, gru_preds)):.2f}\")\nprint(f\"MAPE : {mape(y_true, gru_preds):.2f}%\")\nprint(f\"SMAPE: {smape(y_true, gru_preds):.2f}%\")\nprint(f\"R²   : {r2_score(y_true, gru_preds):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T13:58:36.149118Z","iopub.execute_input":"2025-04-25T13:58:36.149840Z","iopub.status.idle":"2025-04-25T13:59:09.983836Z","shell.execute_reply.started":"2025-04-25T13:58:36.149814Z","shell.execute_reply":"2025-04-25T13:59:09.983204Z"}},"outputs":[{"name":"stderr","text":"2025-04-25 13:58:38.326726: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745589518.610026      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745589518.690162      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Training LSTM model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/masking.py:47: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\nI0000 00:00:1745589532.061336      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14955 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0epoch [00:00, ?epoch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dea9be6e667f4eeeacdc0d50a51d7dfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0batch [00:00, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"I0000 00:00:1745589536.814954     153 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step  \n\n📈 LSTM MAE: 2196.91, RMSE: 3473.00\n\nTraining GRU model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/masking.py:47: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0epoch [00:00, ?epoch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c243e8d59bc4b1daea163b5bcf51b01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0batch [00:00, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step  \n\n📈 GRU MAE: 1963.03, RMSE: 2885.25\n\n🔍 LSTM Metrics:\nMAE  : 2196.91\nRMSE : 3473.00\nMAPE : 48.78%\nSMAPE: 34.72%\nR²   : 0.7893\n\n🔍 GRU Metrics:\nMAE  : 1963.03\nRMSE : 2885.25\nMAPE : 44.55%\nSMAPE: 31.62%\nR²   : 0.8545\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\nfrom pytorch_forecasting.data import GroupNormalizer\nfrom pytorch_forecasting.metrics import SMAPE\nfrom pytorch_lightning import Trainer, seed_everything, LightningModule\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport math\nimport types\n\n# ⚙️ Load dataset\ndf = pd.read_csv(\"/kaggle/input/scor-dataset/updated_scor_raw_df (22).csv\")\n\n# 🧼 Preprocess\nagg_df = df.groupby(['SiteID', 'Year']).agg({\n    'scor_lta': 'mean',\n    'TempC': 'mean',\n    'density': 'mean',\n    'LTA_cm2': 'mean'\n}).reset_index()\n\nagg_df = agg_df.dropna(subset=['scor_lta'])\n\nfor col in ['TempC', 'density', 'LTA_cm2']:\n    agg_df[col] = agg_df.groupby(\"SiteID\")[col].transform(lambda x: x.fillna(x.mean()))\n    agg_df[col] = agg_df[col].fillna(agg_df[col].mean())\n\nagg_df['scor_lta'] = np.log1p(agg_df['scor_lta'])  # log transform\nagg_df[\"time_idx\"] = agg_df[\"Year\"] - agg_df[\"Year\"].min()\nagg_df[\"SiteID\"] = agg_df[\"SiteID\"].astype(str)\n\n# ⏳ Temporal params\nmax_encoder_length = 3\nmax_prediction_length = 1\n\nsite_counts = agg_df.groupby(\"SiteID\")[\"Year\"].nunique()\nvalid_sites = site_counts[site_counts >= (max_encoder_length + max_prediction_length)].index\nagg_df = agg_df[agg_df[\"SiteID\"].isin(valid_sites)]\n\n# 📦 Dataset\ntraining = TimeSeriesDataSet(\n    agg_df,\n    time_idx=\"time_idx\",\n    target=\"scor_lta\",\n    group_ids=[\"SiteID\"],\n    max_encoder_length=max_encoder_length,\n    max_prediction_length=max_prediction_length,\n    static_categoricals=[\"SiteID\"],\n    time_varying_known_reals=[\"time_idx\", \"Year\"],\n    time_varying_unknown_reals=[\"scor_lta\", \"TempC\", \"density\", \"LTA_cm2\"],\n    target_normalizer=GroupNormalizer(groups=[\"SiteID\"]),\n    add_relative_time_idx=True,\n    add_target_scales=True,\n    add_encoder_length=True,\n    allow_missing_timesteps=True\n)\n\n# ✂️ Train/Val split\nval_cutoff = agg_df[\"time_idx\"].max() - max_prediction_length\ntrain_data = agg_df[agg_df[\"time_idx\"] <= val_cutoff]\nval_data = agg_df[agg_df[\"time_idx\"] > val_cutoff]\n\ndef filter_min_years(df, min_years=4):\n    site_counts = df.groupby(\"SiteID\")[\"Year\"].nunique()\n    valid_sites = site_counts[site_counts >= min_years].index\n    return df[df[\"SiteID\"].isin(valid_sites)]\n\ntrain_data = filter_min_years(train_data)\nval_data = filter_min_years(val_data)\n\n# 🔄 Dataloaders\ntrain_ds = TimeSeriesDataSet.from_dataset(training, train_data)\ntrain_dataloader = train_ds.to_dataloader(train=True, batch_size=64, num_workers=0)\n\nval_dataloader = None\nif len(val_data) >= 1:\n    val_ds = TimeSeriesDataSet.from_dataset(training, val_data)\n    val_dataloader = val_ds.to_dataloader(train=False, batch_size=64, num_workers=0)\nelse:\n    print(\"⚠️ Validation set is empty after filtering. Proceeding without validation.\")\n\n# ✅ Lightning wrapper for PL 2.x\nclass TFTLightningWrapper(LightningModule):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n\n    def training_step(self, batch, batch_idx):\n        loss = self.model.training_step(batch, batch_idx)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss = self.model.validation_step(batch, batch_idx)\n        self.log(\"val_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return self.model.configure_optimizers()\n\n    def forward(self, *args, **kwargs):\n        return self.model(*args, **kwargs)\n\n# 🎯 Instantiate raw TFT model\nseed_everything(42)\n\ntft_raw = TemporalFusionTransformer.from_dataset(\n    training,\n    learning_rate=0.03,\n    hidden_size=32,\n    attention_head_size=1,\n    dropout=0.1,\n    hidden_continuous_size=16,\n    output_size=1,\n    loss=SMAPE(),\n    logging_metrics=None,\n)\n\n# ✅ Patch: fix device mismatch in get_attention_mask\ndef patched_get_attention_mask(self, encoder_lengths, decoder_lengths):\n    encoder_mask = torch.arange(self.max_encoder_length, device=encoder_lengths.device)[None, :] < encoder_lengths[:, None]\n    decoder_mask = torch.arange(self.max_prediction_length, device=decoder_lengths.device)[None, :] < decoder_lengths[:, None]\n    return torch.cat((encoder_mask, decoder_mask), dim=1)\n\ntft_raw.get_attention_mask = types.MethodType(patched_get_attention_mask, tft_raw)\n\n# ✅ Wrap for Lightning 2.x training\ntft = TFTLightningWrapper(tft_raw)\n\n# ⚡ Trainer\ntrainer = Trainer(\n    max_epochs=30,\n    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n    devices=1 if torch.cuda.is_available() else None,\n    gradient_clip_val=0.1,\n    enable_progress_bar=True\n)\n\n# 🔁 Fit model\ntrainer.fit(tft, train_dataloader, val_dataloader if val_dataloader else None)\n\n# 📊 Evaluate if possible\nif val_dataloader:\n    predictions, x = tft_raw.predict(val_dataloader, return_x=True)\n    y_true_log = x[\"decoder_target\"].detach().cpu().numpy()\n    y_pred_log = predictions.detach().cpu().numpy()\n\n    y_true = np.expm1(y_true_log).flatten()\n    y_pred = np.expm1(y_pred_log).flatten()\n\n    def smape_score(y_true, y_pred):\n        return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n\n    def mape_score(y_true, y_pred):\n        return 100 * np.mean(np.abs((y_true - y_pred) / y_true))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T13:59:09.984898Z","iopub.execute_input":"2025-04-25T13:59:09.985136Z","iopub.status.idle":"2025-04-25T13:59:10.208199Z","shell.execute_reply.started":"2025-04-25T13:59:09.985118Z","shell.execute_reply":"2025-04-25T13:59:10.207388Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3435297634.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_forecasting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeSeriesDataSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTemporalFusionTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_forecasting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGroupNormalizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_forecasting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMAPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_forecasting'"],"ename":"ModuleNotFoundError","evalue":"No module named 'pytorch_forecasting'","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"! pip install pytorch_forecasting","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install pytorch-lightning==2.1.3 pytorch-forecasting==1.3.0 optuna tqdm --quiet\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install pytorch-lightning==2.1.3 pytorch-forecasting==1.3.0 optuna tqdm --quiet\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the condition dataset\ndf = pd.read_csv(\"/kaggle/input/condition-counts/CREMP_SCOR_Summaries_2023_ConditionCounts.csv\")\n\n# Step 1: Melt condition columns for long-format analysis\ncondition_cols = df.columns[9:]  # All condition columns\nmelted = df.melt(\n    id_vars=['SiteID', 'Site_name', 'sciName'],\n    value_vars=condition_cols,\n    var_name='Condition',\n    value_name='ConditionCount'  # <- Changed from 'Count' to avoid conflict\n)\n\n# Step 2: Remove 0 or NaN condition counts\nmelted['ConditionCount'] = pd.to_numeric(melted['ConditionCount'], errors='coerce')\nmelted = melted[melted['ConditionCount'] > 0]\n\n# Step 3: Top conditions overall\ntop_conditions = melted.groupby('Condition')['ConditionCount'].sum().sort_values(ascending=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=top_conditions.values, y=top_conditions.index, palette='viridis')\nplt.title(\"Most Frequently Observed Coral Conditions\")\nplt.xlabel(\"Total Observations\")\nplt.ylabel(\"Condition\")\nplt.tight_layout()\nplt.show()\n\n# Step 4: Heatmap of conditions per site\nsite_condition = melted.groupby(['Site_name', 'Condition'])['ConditionCount'].sum().reset_index()\npivot_site = site_condition.pivot(index='Site_name', columns='Condition', values='ConditionCount').fillna(0)\n\nplt.figure(figsize=(15, 10))\nsns.heatmap(pivot_site, cmap=\"YlOrRd\", linewidths=0.5)\nplt.title(\"Condition Distribution Across Coral Sites\")\nplt.xlabel(\"Condition\")\nplt.ylabel(\"Site\")\nplt.tight_layout()\nplt.show()\n\n# Step 5: Heatmap of conditions per species\nspecies_condition = melted.groupby(['sciName', 'Condition'])['ConditionCount'].sum().reset_index()\npivot_species = species_condition.pivot(index='sciName', columns='Condition', values='ConditionCount').fillna(0)\n\nplt.figure(figsize=(15, 12))\nsns.heatmap(pivot_species, cmap=\"Blues\", linewidths=0.5)\nplt.title(\"Condition Impact Across Coral Species\")\nplt.xlabel(\"Condition\")\nplt.ylabel(\"Coral Species\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import folium\nfrom folium.plugins import MarkerCluster\nimport seaborn as sns\n\n# Load condition count and station coordinates\ncondition_df = pd.read_csv('/kaggle/input/condition-counts/CREMP_SCOR_Summaries_2023_ConditionCounts.csv')\ncoords_df = pd.read_csv('/kaggle/input/coral-reef-data/coral_csv/CREMP_Stations_2023.csv')\n\n# Ensure numeric columns are properly typed\nfor col in condition_df.columns[9:]:\n    condition_df[col] = pd.to_numeric(condition_df[col], errors='coerce')\n\n# Compute a severity score (simple sum of all conditions per SiteID)\ncondition_df['SeverityScore'] = condition_df.iloc[:, 9:].sum(axis=1)\nsite_severity = condition_df.groupby('SiteID')['SeverityScore'].sum().reset_index()\n\n# Merge with coordinates\nmerged_df = coords_df.merge(site_severity, on='SiteID', how='left').fillna(0)\n\n# Color palette based on severity (normalize scores)\nmax_score = merged_df['SeverityScore'].max()\nmerged_df['Color'] = merged_df['SeverityScore'].apply(lambda x: sns.color_palette(\"Reds\", as_cmap=True)(x / max_score))\n\n# Create folium map\nreef_map = folium.Map(location=[merged_df['latDD'].mean(), merged_df['lonDD'].mean()],\n                      zoom_start=8, tiles=\"cartodbpositron\")\nmarker_cluster = MarkerCluster().add_to(reef_map)\n\n# Add each site to map with severity-based circle marker\nfor _, row in merged_df.iterrows():\n    folium.CircleMarker(\n        location=[row['latDD'], row['lonDD']],\n        radius=6 + (row['SeverityScore'] / max_score) * 10,  # bubble size scaled by severity\n        color=None,\n        fill=True,\n        fill_color=sns.color_palette(\"Reds\", as_cmap=True)(row['SeverityScore'] / max_score),\n        fill_opacity=0.8,\n        popup=(f\"<b>Site:</b> {row['Site_name']}<br>\"\n               f\"<b>Habitat:</b> {row['Habitat']}<br>\"\n               f\"<b>Severity Score:</b> {row['SeverityScore']:.1f}<br>\"\n               f\"<b>Depth:</b> {row['Depth_ft']} ft\"),\n    ).add_to(marker_cluster)\nreef_map\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import folium\nfrom folium.plugins import MarkerCluster\nfrom branca.colormap import LinearColormap\n\n# Setup severity color scale\nseverity_max = merged_df['SeverityScore'].max()\nseverity_min = merged_df['SeverityScore'].min()\ncolormap = LinearColormap(\n    colors=['green', 'yellow', 'orange', 'red', 'darkred'],\n    index=[severity_min, severity_max * 0.25, severity_max * 0.5, severity_max * 0.75, severity_max],\n    vmin=severity_min,\n    vmax=severity_max,\n    caption='Coral Condition Severity Score'\n)\n\n# Create map\nreef_map = folium.Map(location=[merged_df['latDD'].mean(), merged_df['lonDD'].mean()],\n                      zoom_start=8, tiles='cartodbpositron')\n\nmarker_cluster = MarkerCluster().add_to(reef_map)\n\n# Add markers with color/size based on severity\nfor _, row in merged_df.iterrows():\n    severity = row['SeverityScore']\n    folium.CircleMarker(\n        location=[row['latDD'], row['lonDD']],\n        radius=4 + (severity / severity_max) * 10,\n        color=colormap(severity),\n        fill=True,\n        fill_color=colormap(severity),\n        fill_opacity=0.85,\n        popup=folium.Popup(\n            html=(\n                f\"<b>📍 Site:</b> {row['Site_name']}<br>\"\n                f\"<b>🏝️ Habitat:</b> {row['Habitat']}<br>\"\n                f\"<b>🌡️ Depth:</b> {row['Depth_ft']} ft<br>\"\n                f\"<b>🔥 Severity Score:</b> <b style='color:{colormap(severity)}'>{severity:.1f}</b>\"\n            ),\n            max_width=250\n        )\n    ).add_to(marker_cluster)\n\n# Add color scale to map\ncolormap.add_to(reef_map)\n\nreef_map\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport folium\nfrom folium.plugins import HeatMap\n\n# --- STEP 1: Load and process data ---\n\n\n\n# Convert all condition columns to numeric safely\nfor col in condition_df.columns[9:]:\n    condition_df[col] = pd.to_numeric(condition_df[col], errors='coerce')\n\n# Calculate severity score = sum of all condition counts\ncondition_df['SeverityScore'] = condition_df.iloc[:, 9:].sum(axis=1)\n\n# Aggregate severity by SiteID\nsite_severity = condition_df.groupby('SiteID')['SeverityScore'].sum().reset_index()\n\n# Merge severity scores with station coordinates\nmerged_df = coords_df.merge(site_severity, on='SiteID', how='left').fillna(0)\n\n# --- STEP 2: Prepare data for heatmap ---\n\n# Extract necessary columns and drop rows with missing values\nheatmap_df = merged_df[['latDD', 'lonDD', 'SeverityScore']].dropna()\n\n# Ensure correct column names and datatypes\nheatmap_df.columns = heatmap_df.columns.map(str)\nheatmap_df = heatmap_df.astype(float)\n\n# Create list of [lat, lon, severity] points for the heatmap\nheat_data = heatmap_df[['latDD', 'lonDD', 'SeverityScore']].values.tolist()\n\n# --- STEP 3: Generate Folium HeatMap ---\n\n# Create base map centered on the region\nheat_map = folium.Map(\n    location=[heatmap_df['latDD'].mean(), heatmap_df['lonDD'].mean()],\n    zoom_start=7,\n    tiles=\"cartodb dark_matter\"\n)\n\n# Add heatmap layer\nHeatMap(\n    data=heat_data,\n    radius=18,\n    blur=15,\n    max_zoom=13,\n    min_opacity=0.4,\n    gradient={\n        0.2: 'green',\n        0.4: 'yellow',\n        0.6: 'orange',\n        0.8: 'red',\n        1.0: 'darkred'\n    }\n).add_to(heat_map)\n\n# Show the map in the notebook\nheat_map\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install biopython\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T08:39:57.818742Z","iopub.execute_input":"2025-04-23T08:39:57.818983Z"}},"outputs":[{"name":"stdout","text":"Collecting biopython\n  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->biopython) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->biopython) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->biopython) (2024.2.0)\nDownloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: biopython\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!pip install biopython\nfrom Bio import SeqIO\nimport pandas as pd\n\nrecords = list(SeqIO.parse(\"/kaggle/input/fastq-output/fastq_output/SRR20727752_1.fastq\", \"fastq\"))\ndf = pd.DataFrame({\n    \"Read_ID\": [rec.id for rec in records],\n    \"Sequence\": [str(rec.seq) for rec in records],\n    \"Quality_Scores\": [rec.letter_annotations[\"phred_quality\"] for rec in records],\n    \"Avg_Quality\": [sum(rec.letter_annotations[\"phred_quality\"])/len(rec) for rec in records],\n    \"Length\": [len(rec.seq) for rec in records]\n})\ndf.to_csv(\"SRR20727787_2_converted.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T11:55:14.197571Z","iopub.execute_input":"2025-04-25T11:55:14.197811Z","iopub.status.idle":"2025-04-25T11:55:19.863166Z","shell.execute_reply.started":"2025-04-25T11:55:14.197793Z","shell.execute_reply":"2025-04-25T11:55:19.862303Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: biopython in /usr/local/lib/python3.11/dist-packages (1.85)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->biopython) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->biopython) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->biopython) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# Load the FASTQ CSV file\ndf = pd.read_csv(\"/kaggle/working/SRR20727787_2_converted.csv\")  # <- update path if needed\n\n# Label: Favorable = Avg_Quality > 30\ndf['Label'] = (df['Avg_Quality'] > 30).astype(int)\n\n# Encode DNA sequence (A=0, T=1, G=2, C=3, N=4)\nvocab = {'A': 0, 'T': 1, 'G': 2, 'C': 3, 'N': 4}\ndef encode_sequence(seq, max_len=300):\n    return [vocab.get(base, 4) for base in seq[:max_len]] + [0]*(max_len - len(seq))\n\ndf['Encoded_Seq'] = df['Sequence'].apply(lambda x: encode_sequence(x))\n\n# Split into train/test\nX_train, X_test, y_train, y_test = train_test_split(df['Encoded_Seq'].tolist(), df['Label'].tolist(), test_size=0.2)\n\n# Torch Dataset\nclass DNADataset(Dataset):\n    def __init__(self, sequences, labels):\n        self.X = torch.tensor(sequences, dtype=torch.long)\n        self.y = torch.tensor(labels, dtype=torch.long)\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n\ntrain_ds = DNADataset(X_train, y_train)\ntest_ds = DNADataset(X_test, y_test)\ntrain_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\ntest_dl = DataLoader(test_ds, batch_size=32)\n\n# Transformer-based model\nclass SimpleDNATransformer(nn.Module):\n    def __init__(self, vocab_size=5, d_model=64, nhead=4, num_layers=2, max_len=300):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.classifier = nn.Linear(d_model, 2)\n\n    def forward(self, x):\n        x = self.embedding(x).permute(1, 0, 2)  # (seq_len, batch, d_model)\n        x = self.encoder(x).permute(1, 2, 0)    # (batch, d_model, seq_len)\n        x = self.pool(x).squeeze(-1)            # (batch, d_model)\n        return self.classifier(x)               # (batch, 2)\n\n# Initialize\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SimpleDNATransformer().to(device)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# Training loop (few epochs)\nfor epoch in range(3):\n    model.train()\n    total_loss = 0\n    for xb, yb in train_dl:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        preds = model(xb)\n        loss = loss_fn(preds, yb)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")\n\n# Evaluation\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for xb, yb in test_dl:\n        xb, yb = xb.to(device), yb.to(device)\n        preds = model(xb)\n        predicted = preds.argmax(1)\n        correct += (predicted == yb).sum().item()\n        total += yb.size(0)\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T11:55:19.864646Z","iopub.execute_input":"2025-04-25T11:55:19.864938Z","iopub.status.idle":"2025-04-25T11:56:22.490625Z","shell.execute_reply.started":"2025-04-25T11:55:19.864910Z","shell.execute_reply":"2025-04-25T11:56:22.489918Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Loss: 158.3539\nEpoch 2 | Loss: 155.6938\nEpoch 3 | Loss: 153.5260\nTest Accuracy: 94.80%\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip uninstall torch\n!pip install torch==2.0.1+cpu torchvision==0.15.2+cpu -f https://download.pytorch.org/whl/torch_stable.html\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:14:16.146077Z","iopub.execute_input":"2025-04-23T09:14:16.146361Z","iopub.status.idle":"2025-04-23T09:27:36.596263Z","shell.execute_reply.started":"2025-04-23T09:14:16.146337Z","shell.execute_reply":"2025-04-23T09:27:36.595460Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.5.1+cu124\nUninstalling torch-2.5.1+cu124:\n  Would remove:\n    /usr/local/bin/convert-caffe2-to-onnx\n    /usr/local/bin/convert-onnx-to-caffe2\n    /usr/local/bin/torchfrtrace\n    /usr/local/bin/torchrun\n    /usr/local/lib/python3.11/dist-packages/functorch/*\n    /usr/local/lib/python3.11/dist-packages/torch-2.5.1+cu124.dist-info/*\n    /usr/local/lib/python3.11/dist-packages/torch/*\n    /usr/local/lib/python3.11/dist-packages/torchgen/*\nProceed (Y/n)? ^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0mLooking in links: https://download.pytorch.org/whl/torch_stable.html\nCollecting torch==2.0.1+cpu\n  Downloading https://download.pytorch.org/whl/cpu/torch-2.0.1%2Bcpu-cp311-cp311-linux_x86_64.whl (195.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchvision==0.15.2+cpu\n  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.15.2%2Bcpu-cp311-cp311-linux_x86_64.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cpu) (3.18.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cpu) (4.13.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cpu) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cpu) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cpu) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cpu) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cpu) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cpu) (11.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1+cpu) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cpu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cpu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cpu) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cpu) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cpu) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cpu) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cpu) (2025.1.31)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1+cpu) (1.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.15.2+cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.15.2+cpu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.15.2+cpu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.15.2+cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.15.2+cpu) (2024.2.0)\nInstalling collected packages: torch, torchvision\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu124\n    Uninstalling torch-2.5.1+cu124:\n      Successfully uninstalled torch-2.5.1+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu124\n    Uninstalling torchvision-0.20.1+cu124:\n      Successfully uninstalled torchvision-0.20.1+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.5.1 requires torch>=2.1.0, but you have torch 2.0.1+cpu which is incompatible.\ntorchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.0.1+cpu which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-2.0.1+cpu torchvision-0.15.2+cpu\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}